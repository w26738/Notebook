{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** <font size=4>支持向量机SVM</font>  ** \n",
    "\n",
    "Ref:  \n",
    "1. Gavin Hackeling, Mastering Machine Learning with scikit-learn, 2014\n",
    "2. https://www.zhihu.com/question/21094489\n",
    "3. http://blog.csdn.net/macyang/article/details/38782399/\n",
    "4. http://www.cnblogs.com/steven-yang/p/5658362.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是SVM：\n",
    "1. Support Vector Machine: 一个普通的SVM就是一条直线罢了，用来完美划分linearly separable的两类。\n",
    "2. 但这又不是一条普通的直线，这是无数条可以分类的直线当中最完美的，因为它恰好在两个类的中间，距离两个类的点都一样远。所谓的Support vector就是这些离分界线最近的“点”\n",
    "3. 如果是高维的点，SVM的分界线就是平面或者超平面。\n",
    "4. 另外，分类的那条线不一定是直线，还有可能是曲线，我们通过某些函数来转换，就可以转化成刚才的哪种多维的分类问题，这个就是核函数的思想。\n",
    "\n",
    "核心思想：\n",
    "1. SVM的目的是要找到一个线性分类的最佳超平面 f(x)=xwT+b=0。求 w 和 b。\n",
    "2. 首先通过两个分类的最近点，找到f(x)的约束条件。\n",
    "3. 有了约束条件，就可以通过拉格朗日乘子法和KKT条件来求解，这时，问题变成了求拉格朗日乘子αi 和 b。\n",
    "4. 对于异常点的情况，加入松弛变量ξ来处理。\n",
    "5. 使用SMO来求拉格朗日乘子αi和b。这时，我们会发现有些αi=0，这些点就可以不用在分类器中考虑了。\n",
    "6. 不用求w了，可以使用拉格朗日乘子αi和b作为分类器的参数。\n",
    "7. 非线性分类的问题：映射到高维度、使用核函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  27 out of  27 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.945\n",
      "Best parameters set:\n",
      "\tclf__C: 3\n",
      "\tclf__gamma: 0.005\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.97      0.97      1722\n",
      "        1.0       0.97      0.98      0.97      1965\n",
      "        2.0       0.94      0.94      0.94      1724\n",
      "        3.0       0.93      0.91      0.92      1751\n",
      "        4.0       0.94      0.94      0.94      1700\n",
      "        5.0       0.93      0.93      0.93      1620\n",
      "        6.0       0.95      0.97      0.96      1693\n",
      "        7.0       0.96      0.95      0.95      1841\n",
      "        8.0       0.94      0.92      0.93      1742\n",
      "        9.0       0.91      0.93      0.92      1742\n",
      "\n",
      "avg / total       0.95      0.95      0.95     17500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 例子：MNIST数据集分类\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mnist = fetch_mldata('mnist', data_home = r'D:\\Code\\GitHub\\notebook\\datasets\\mnist')\n",
    "X, y = mnist.data, mnist.target\n",
    "X = X/255*2 - 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('clf', SVC(kernel='rbf', gamma=0.01, C=100))\n",
    "])\n",
    "parameters = {\n",
    "    'clf__gamma': (0.005, 0.01, 0.1),\n",
    "    'clf__C': (1, 3, 10),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=2, verbose=1, scoring='accuracy')\n",
    "grid_search.fit(X_train[:2000], y_train[:2000])\n",
    "print('Best score: %0.3f' %grid_search.best_score_)\n",
    "print('Best parameters set:')\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "predictions = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
